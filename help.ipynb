{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # 데이터 로드 및 준비 (가상의 예제 데이터)\n",
    "# X, y = np.random.rand(100, 20), np.random.randint(0, 2, 100)  # 20차원 데이터\n",
    "\n",
    "# # 데이터 스케일링\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # PCA 적용\n",
    "# pca = PCA(n_components=10)  # 10개의 주요 성분을 선택\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # 데이터 분할\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 모델 정의\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # 모델 훈련\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # 예측\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # 성능 평가\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'정확도: {accuracy}')\n",
    "\n",
    "# # PCA 변환 후 데이터 확인\n",
    "# print('원본 데이터의 차원:', X.shape[1])\n",
    "# print('PCA 적용 후 데이터의 차원:', X_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류, 회귀 모델\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "# from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # Initialize individual regression models\n",
    "# regression_models = {\n",
    "#     'Linear Regression': LinearRegression(),\n",
    "#     'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "#     'Bagging Regressor': BaggingRegressor(n_estimators=10, random_state=42),\n",
    "#     'XGBoost Regressor': XGBRegressor(n_estimators=100, random_state=42),\n",
    "#     'ElasticNet Regressor': ElasticNet(random_state=42),\n",
    "#     'K-Nearest Neighbors Regressor': KNeighborsRegressor()\n",
    "# }\n",
    "\n",
    "# # Initialize individual classification models\n",
    "# classification_models = {\n",
    "#     'Logistic Regression': LogisticRegression(),\n",
    "#     'Random Forest Classifier': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#     'Bagging Classifier': BaggingClassifier(n_estimators=10, random_state=42),\n",
    "#     'Gradient Boosting Classifier': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "#     'Support Vector Classifier': SVC(probability=True, random_state=42),\n",
    "#     'K-Nearest Neighbors Classifier': KNeighborsClassifier(),\n",
    "#     'XGBoost Classifier': XGBClassifier(n_estimators=100, random_state=42)\n",
    "# }\n",
    "\n",
    "# # Directory to save model weights\n",
    "# save_model_path = './save_models_all'\n",
    "# os.makedirs(save_model_path, exist_ok=True)  # Create the directory if it does not exist\n",
    "\n",
    "# # Dictionary to store the performance metrics for each model\n",
    "# regression_metrics = {\n",
    "#     'Model': [],\n",
    "#     'MSE': [],\n",
    "#     'MASE': [],\n",
    "#     'R2': [],\n",
    "#     'RMSE': [],\n",
    "#     'Training Time': []\n",
    "# }\n",
    "\n",
    "# classification_metrics = {\n",
    "#     'Model': [],\n",
    "#     'Accuracy': [],\n",
    "#     'Training Time': []\n",
    "# }\n",
    "\n",
    "# # Train and evaluate each regression model\n",
    "# for name, model in regression_models.items():\n",
    "#     start_time = time.time()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     training_time = end_time - start_time\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mase = mean_absolute_scaled_error(y_test, y_pred)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "\n",
    "#     regression_metrics['Model'].append(name)\n",
    "#     regression_metrics['MSE'].append(mse)\n",
    "#     regression_metrics['MASE'].append(mase)\n",
    "#     regression_metrics['R2'].append(r2)\n",
    "#     regression_metrics['RMSE'].append(rmse)\n",
    "#     regression_metrics['Training Time'].append(training_time)\n",
    "\n",
    "#     print(f'{name} - MSE: {mse}')\n",
    "#     print(f'{name} - MASE: {mase}')\n",
    "#     print(f'{name} - R2: {r2}')\n",
    "#     print(f'{name} - RMSE: {rmse}')\n",
    "#     print(f'{name} - Training Time: {training_time}')\n",
    "\n",
    "#     model_filename = os.path.join(save_model_path, f'{name}_model.pkl')\n",
    "#     joblib.dump(model, model_filename)\n",
    "#     print(f'Model saved as {model_filename}')\n",
    "\n",
    "# # Train and evaluate each classification model\n",
    "# for name, model in classification_models.items():\n",
    "#     start_time = time.time()\n",
    "#     model.fit(X_train_class, y_train_class)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     training_time = end_time - start_time\n",
    "#     y_pred_class = model.predict(X_test_class)\n",
    "\n",
    "#     accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "\n",
    "#     classification_metrics['Model'].append(name)\n",
    "#     classification_metrics['Accuracy'].append(accuracy)\n",
    "#     classification_metrics['Training Time'].append(training_time)\n",
    "\n",
    "#     print(f'{name} - Accuracy: {accuracy}')\n",
    "#     print(f'{name} - Training Time: {training_time}')\n",
    "\n",
    "#     model_filename = os.path.join(save_model_path, f'{name}_model.pkl')\n",
    "#     joblib.dump(model, model_filename)\n",
    "#     print(f'Model saved as {model_filename}')\n",
    "\n",
    "# # Convert the metrics dictionaries to DataFrames for a cleaner display\n",
    "# regression_metrics_df = pd.DataFrame(regression_metrics)\n",
    "# classification_metrics_df = pd.DataFrame(classification_metrics)\n",
    "\n",
    "# print('Regression Metrics:')\n",
    "# print(regression_metrics_df)\n",
    "\n",
    "# print('Classification Metrics:')\n",
    "# print(classification_metrics_df)\n",
    "\n",
    "# # Save the metrics to CSV files\n",
    "# regression_metrics_df.to_csv('regression_results.csv', index=False)\n",
    "# classification_metrics_df.to_csv('classification_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드 서치\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define parameter grids\n",
    "# param_grids = {\n",
    "#     'Random Forest Regressor': {\n",
    "#         'n_estimators': [50, 100, 200],\n",
    "#         'max_depth': [None, 10, 20, 30],\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'min_samples_leaf': [1, 2, 4],\n",
    "#         'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#         'bootstrap': [True, False]\n",
    "#     },\n",
    "#     'Random Forest Classifier': {\n",
    "#         'n_estimators': [50, 100, 200],\n",
    "#         'max_depth': [None, 10, 20, 30],\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'min_samples_leaf': [1, 2, 4],\n",
    "#         'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#         'bootstrap': [True, False]\n",
    "#     },\n",
    "#     'Bagging Regressor': {\n",
    "#         'n_estimators': [10, 50, 100],\n",
    "#         'max_samples': [0.5, 0.8, 1.0],\n",
    "#         'max_features': [0.5, 0.8, 1.0],\n",
    "#         'bootstrap': [True, False]\n",
    "#     },\n",
    "#     'Bagging Classifier': {\n",
    "#         'n_estimators': [10, 50, 100],\n",
    "#         'max_samples': [0.5, 0.8, 1.0],\n",
    "#         'max_features': [0.5, 0.8, 1.0],\n",
    "#         'bootstrap': [True, False]\n",
    "#     },\n",
    "#     'XGBoost Regressor': {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'max_depth': [3, 5, 7],\n",
    "#         'min_child_weight': [1, 3, 5],\n",
    "#         'subsample': [0.8, 1.0],\n",
    "#         'colsample_bytree': [0.8, 1.0],\n",
    "#         'gamma': [0, 0.1, 0.2]\n",
    "#     },\n",
    "#     'XGBoost Classifier': {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'max_depth': [3, 5, 7],\n",
    "#         'min_child_weight': [1, 3, 5],\n",
    "#         'subsample': [0.8, 1.0],\n",
    "#         'colsample_bytree': [0.8, 1.0],\n",
    "#         'gamma': [0, 0.1, 0.2]\n",
    "#     },\n",
    "#     'ElasticNet Regressor': {\n",
    "#         'alpha': [0.1, 1.0, 10.0],\n",
    "#         'l1_ratio': [0.1, 0.5, 0.9]\n",
    "#     },\n",
    "#     'Logistic Regression': {\n",
    "#         'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "#         'penalty': ['l1', 'l2'],\n",
    "#         'solver': ['liblinear', 'newton-cg', 'lbfgs', 'saga']\n",
    "#     },\n",
    "#     'Gradient Boosting Classifier': {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'max_depth': [3, 5, 7],\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'min_samples_leaf': [1, 2, 4],\n",
    "#         'subsample': [0.8, 1.0]\n",
    "#     },\n",
    "#     'Support Vector Classifier': {\n",
    "#         'C': [0.1, 1.0, 10.0],\n",
    "#         'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#         'gamma': [0.001, 0.01, 0.1, 1.0],\n",
    "#         'degree': [2, 3, 4]\n",
    "#     },\n",
    "#     'K-Nearest Neighbors Classifier': {\n",
    "#         'n_neighbors': [3, 5, 7, 10],\n",
    "#         'weights': ['uniform', 'distance'],\n",
    "#         'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "#     },\n",
    "#     'K-Nearest Neighbors Regressor': {\n",
    "#         'n_neighbors': [3, 5, 7, 10],\n",
    "#         'weights': ['uniform', 'distance'],\n",
    "#         'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Example usage of GridSearchCV for Random Forest Regressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Choose a model and parameter grid\n",
    "# model_name = 'Random Forest Regressor'\n",
    "# model = RandomForestRegressor()\n",
    "# param_grid = param_grids[model_name]\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# # Fit GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print best parameters and best score\n",
    "# print(f'Best Parameters for {model_name}: {grid_search.best_params_}')\n",
    "# print(f'Best Score for {model_name}: {-grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
